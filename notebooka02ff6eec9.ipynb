{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Insepcting the Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport re\nfrom nltk.corpus import stopwords\nimport time\nfrom tensorflow.python.layers.core import Dense\nfrom tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\nprint('TensorFlow Version: {}'.format(tf.__version__))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T03:32:36.455228Z","iopub.execute_input":"2021-12-27T03:32:36.455513Z","iopub.status.idle":"2021-12-27T03:32:36.461949Z","shell.execute_reply.started":"2021-12-27T03:32:36.455471Z","shell.execute_reply":"2021-12-27T03:32:36.460997Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"reviews = pd.read_csv(\"../input/amazon-fine-food-reviews/Reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:37:26.876167Z","iopub.execute_input":"2021-12-27T03:37:26.876493Z","iopub.status.idle":"2021-12-27T03:37:31.062938Z","shell.execute_reply.started":"2021-12-27T03:37:26.876441Z","shell.execute_reply":"2021-12-27T03:37:31.062057Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"reviews.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:37:33.623578Z","iopub.execute_input":"2021-12-27T03:37:33.623871Z","iopub.status.idle":"2021-12-27T03:37:33.629114Z","shell.execute_reply.started":"2021-12-27T03:37:33.623838Z","shell.execute_reply":"2021-12-27T03:37:33.628544Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"reviews.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:37:36.966201Z","iopub.execute_input":"2021-12-27T03:37:36.966935Z","iopub.status.idle":"2021-12-27T03:37:36.980733Z","shell.execute_reply.started":"2021-12-27T03:37:36.966898Z","shell.execute_reply":"2021-12-27T03:37:36.979744Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"reviews.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:37:41.540127Z","iopub.execute_input":"2021-12-27T03:37:41.540985Z","iopub.status.idle":"2021-12-27T03:37:41.893666Z","shell.execute_reply.started":"2021-12-27T03:37:41.540930Z","shell.execute_reply":"2021-12-27T03:37:41.892873Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"reviews = reviews.dropna()\nreviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n                        'Score','Time'], 1)\nreviews = reviews.reset_index(drop=True) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:38:42.512835Z","iopub.execute_input":"2021-12-27T03:38:42.513290Z","iopub.status.idle":"2021-12-27T03:38:42.519825Z","shell.execute_reply.started":"2021-12-27T03:38:42.513238Z","shell.execute_reply":"2021-12-27T03:38:42.518977Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"reviews.head()","metadata":{"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(\"Review #\",i+1)\n    print(reviews.Summary[i])\n    print(reviews.Text[i])\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:38:49.244387Z","iopub.execute_input":"2021-12-27T03:38:49.244942Z","iopub.status.idle":"2021-12-27T03:38:49.254174Z","shell.execute_reply.started":"2021-12-27T03:38:49.244906Z","shell.execute_reply":"2021-12-27T03:38:49.253213Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# 2.Preparing the Data","metadata":{}},{"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:38:52.174375Z","iopub.execute_input":"2021-12-27T03:38:52.174661Z","iopub.status.idle":"2021-12-27T03:38:52.186578Z","shell.execute_reply.started":"2021-12-27T03:38:52.174632Z","shell.execute_reply":"2021-12-27T03:38:52.185712Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"def clean_text(text, remove_stopwords = True):\n    \n    text = text.lower()\n    \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n        text = \" \".join(text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:39:01.992321Z","iopub.execute_input":"2021-12-27T03:39:01.993411Z","iopub.status.idle":"2021-12-27T03:39:02.002845Z","shell.execute_reply.started":"2021-12-27T03:39:01.993355Z","shell.execute_reply":"2021-12-27T03:39:02.002250Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"clean_text(\"That's a great movie,Can you believe it?I've.But you may not.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:39:06.465666Z","iopub.execute_input":"2021-12-27T03:39:06.467646Z","iopub.status.idle":"2021-12-27T03:39:06.473586Z","shell.execute_reply.started":"2021-12-27T03:39:06.467608Z","shell.execute_reply":"2021-12-27T03:39:06.472986Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"clean_summaries = []\nfor summary in reviews.Summary:\n    clean_summaries.append(clean_text(summary, remove_stopwords=False))\nprint(\"Summaries are complete.\")\n\nclean_texts = []\nfor text in reviews.Text:\n    clean_texts.append(clean_text(text))\nprint(\"Texts are complete.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:39:12.175829Z","iopub.execute_input":"2021-12-27T03:39:12.176408Z","iopub.status.idle":"2021-12-27T03:41:26.085935Z","shell.execute_reply.started":"2021-12-27T03:39:12.176346Z","shell.execute_reply":"2021-12-27T03:41:26.085250Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(\"Clean Review #\",i+1)\n    print(clean_summaries[i])\n    print(clean_texts[i])\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:41:32.305059Z","iopub.execute_input":"2021-12-27T03:41:32.305320Z","iopub.status.idle":"2021-12-27T03:41:32.314065Z","shell.execute_reply.started":"2021-12-27T03:41:32.305290Z","shell.execute_reply":"2021-12-27T03:41:32.313186Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"def count_words(count_dict, text):\n    for sentence in text:\n        for word in sentence.split():\n            if word not in count_dict:\n                count_dict[word] = 1\n            else:\n                count_dict[word] += 1","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:41:35.154100Z","iopub.execute_input":"2021-12-27T03:41:35.154384Z","iopub.status.idle":"2021-12-27T03:41:35.159886Z","shell.execute_reply.started":"2021-12-27T03:41:35.154351Z","shell.execute_reply":"2021-12-27T03:41:35.159039Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"mydict = {}\ncount_words(mydict, [\"that is a great great great dog\",\"you have a great dog\"])\nmydict","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:41:37.579393Z","iopub.execute_input":"2021-12-27T03:41:37.579713Z","iopub.status.idle":"2021-12-27T03:41:37.586878Z","shell.execute_reply.started":"2021-12-27T03:41:37.579681Z","shell.execute_reply":"2021-12-27T03:41:37.585728Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"word_counts = {}\ncount_words(word_counts, clean_summaries)\ncount_words(word_counts, clean_texts)\nprint(\"Size of Vocabulary:\", len(word_counts))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:41:41.334121Z","iopub.execute_input":"2021-12-27T03:41:41.334399Z","iopub.status.idle":"2021-12-27T03:41:50.583305Z","shell.execute_reply.started":"2021-12-27T03:41:41.334369Z","shell.execute_reply":"2021-12-27T03:41:50.582333Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"word_counts[\"hero\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:42:01.124536Z","iopub.execute_input":"2021-12-27T03:42:01.124839Z","iopub.status.idle":"2021-12-27T03:42:01.131298Z","shell.execute_reply.started":"2021-12-27T03:42:01.124807Z","shell.execute_reply":"2021-12-27T03:42:01.130409Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nwith open('../input/numberbatchen1703txtgz/numberbatch-en.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split(' ')\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding\n\nprint('Word embeddings:', len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:42:02.734654Z","iopub.execute_input":"2021-12-27T03:42:02.734931Z","iopub.status.idle":"2021-12-27T03:42:52.398176Z","shell.execute_reply.started":"2021-12-27T03:42:02.734903Z","shell.execute_reply":"2021-12-27T03:42:52.397045Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"embeddings_index[\"hero\"].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:09.177820Z","iopub.execute_input":"2021-12-27T03:47:09.178346Z","iopub.status.idle":"2021-12-27T03:47:09.184089Z","shell.execute_reply.started":"2021-12-27T03:47:09.178292Z","shell.execute_reply":"2021-12-27T03:47:09.183115Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"missing_words = 0\nthreshold = 20\n\nfor word, count in word_counts.items():\n    if count > threshold:\n        if word not in embeddings_index:\n            missing_words += 1\n            \nmissing_ratio = round(missing_words/len(word_counts),4)*100\n            \nprint(\"Number of words missing from CN:\", missing_words)\nprint(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:11.167559Z","iopub.execute_input":"2021-12-27T03:47:11.168038Z","iopub.status.idle":"2021-12-27T03:47:11.215779Z","shell.execute_reply.started":"2021-12-27T03:47:11.167973Z","shell.execute_reply":"2021-12-27T03:47:11.214868Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"missing_words = []\nfor word, count in word_counts.items():\n    if count > threshold and word not in embeddings_index:\n        missing_words.append((word,count))\nmissing_words[:30]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:14.130578Z","iopub.execute_input":"2021-12-27T03:47:14.130841Z","iopub.status.idle":"2021-12-27T03:47:14.244116Z","shell.execute_reply.started":"2021-12-27T03:47:14.130813Z","shell.execute_reply":"2021-12-27T03:47:14.243188Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"vocab_to_int = {} \n# Index words from 0\nvalue = 0\nfor word, count in word_counts.items():\n    if count >= threshold or word in embeddings_index:\n        vocab_to_int[word] = value\n        value += 1\n\n# Special tokens that will be added to our vocab\ncodes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n\n# Add codes to vocab\nfor code in codes:\n    vocab_to_int[code] = len(vocab_to_int)\n\n# Dictionary to convert integers to words\nint_to_vocab = {}\nfor word, value in vocab_to_int.items():\n    int_to_vocab[value] = word\n\nusage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n\nprint(\"Total number of unique words:\", len(word_counts))\nprint(\"Number of words we will use:\", len(vocab_to_int))\nprint(\"Percent of words we will use: {}%\".format(usage_ratio))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:17.419446Z","iopub.execute_input":"2021-12-27T03:47:17.419720Z","iopub.status.idle":"2021-12-27T03:47:17.532085Z","shell.execute_reply.started":"2021-12-27T03:47:17.419691Z","shell.execute_reply":"2021-12-27T03:47:17.531175Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# Need to use 300 for embedding dimensions to match CN's vectors.\nembedding_dim = 300\nnb_words = len(vocab_to_int)\n\n# Create matrix with default values of zero\nword_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\nfor word, i in vocab_to_int.items():\n    if word in embeddings_index:\n        word_embedding_matrix[i] = embeddings_index[word]\n    else:\n        # If word not in CN, create a random embedding for it\n        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n        embeddings_index[word] = new_embedding\n        word_embedding_matrix[i] = new_embedding\n\n# Check if value matches len(vocab_to_int)\nprint(len(word_embedding_matrix))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:21.690470Z","iopub.execute_input":"2021-12-27T03:47:21.690768Z","iopub.status.idle":"2021-12-27T03:47:21.926970Z","shell.execute_reply.started":"2021-12-27T03:47:21.690739Z","shell.execute_reply":"2021-12-27T03:47:21.926380Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def convert_to_ints(text, word_count, unk_count, eos=False):\n    '''Convert words in text to an integer.\n       If word is not in vocab_to_int, use UNK's integer.\n       Total the number of words and UNKs.\n       Add EOS token to the end of texts'''\n    ints = []\n    for sentence in text:\n        sentence_ints = []\n        for word in sentence.split():\n            word_count += 1\n            if word in vocab_to_int:\n                sentence_ints.append(vocab_to_int[word])\n            else:\n                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n                unk_count += 1\n        if eos:\n            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n        ints.append(sentence_ints)\n    return ints, word_count, unk_count","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:37.476086Z","iopub.execute_input":"2021-12-27T03:47:37.476592Z","iopub.status.idle":"2021-12-27T03:47:37.485659Z","shell.execute_reply.started":"2021-12-27T03:47:37.476541Z","shell.execute_reply":"2021-12-27T03:47:37.484713Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"word_count = 0\nunk_count = 0\n\nint_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\nint_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n\nunk_percent = round(unk_count/word_count,4)*100\n\nprint(\"Total number of words in headlines:\", word_count)\nprint(\"Total number of UNKs in headlines:\", unk_count)\nprint(\"Percent of words that are UNK: {}%\".format(unk_percent))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:40.660808Z","iopub.execute_input":"2021-12-27T03:47:40.661100Z","iopub.status.idle":"2021-12-27T03:47:54.863508Z","shell.execute_reply.started":"2021-12-27T03:47:40.661066Z","shell.execute_reply":"2021-12-27T03:47:54.862646Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"int_summaries[:3]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:54.865014Z","iopub.execute_input":"2021-12-27T03:47:54.865251Z","iopub.status.idle":"2021-12-27T03:47:54.871447Z","shell.execute_reply.started":"2021-12-27T03:47:54.865221Z","shell.execute_reply":"2021-12-27T03:47:54.870433Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"def create_lengths(text):\n    '''Create a data frame of the sentence lengths from a text'''\n    lengths = []\n    for sentence in text:\n        lengths.append(len(sentence))\n    return pd.DataFrame(lengths, columns=['counts'])","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:47:54.872858Z","iopub.execute_input":"2021-12-27T03:47:54.873356Z","iopub.status.idle":"2021-12-27T03:47:54.883272Z","shell.execute_reply.started":"2021-12-27T03:47:54.873322Z","shell.execute_reply":"2021-12-27T03:47:54.882458Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"create_lengths(int_summaries[:3])","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:48:19.870278Z","iopub.execute_input":"2021-12-27T03:48:19.871142Z","iopub.status.idle":"2021-12-27T03:48:19.879295Z","shell.execute_reply.started":"2021-12-27T03:48:19.871105Z","shell.execute_reply":"2021-12-27T03:48:19.878610Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"lengths_summaries = create_lengths(int_summaries)\nlengths_texts = create_lengths(int_texts)\n\nprint(\"Summaries:\")\nprint(lengths_summaries.describe())\nprint()\nprint(\"Texts:\")\nprint(lengths_texts.describe())","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:48:23.678576Z","iopub.execute_input":"2021-12-27T03:48:23.678917Z","iopub.status.idle":"2021-12-27T03:48:24.375123Z","shell.execute_reply.started":"2021-12-27T03:48:23.678882Z","shell.execute_reply":"2021-12-27T03:48:24.374198Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# Inspect the length of texts\nprint(np.percentile(lengths_texts.counts, 89.5))\nprint(np.percentile(lengths_texts.counts, 95))\nprint(np.percentile(lengths_texts.counts, 99))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:48:33.081291Z","iopub.execute_input":"2021-12-27T03:48:33.081582Z","iopub.status.idle":"2021-12-27T03:48:33.111257Z","shell.execute_reply.started":"2021-12-27T03:48:33.081549Z","shell.execute_reply":"2021-12-27T03:48:33.110537Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# Inspect the length of summaries\nprint(np.percentile(lengths_summaries.counts, 90))\nprint(np.percentile(lengths_summaries.counts, 95))\nprint(np.percentile(lengths_summaries.counts, 99))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:48:41.849988Z","iopub.execute_input":"2021-12-27T03:48:41.850252Z","iopub.status.idle":"2021-12-27T03:48:41.874371Z","shell.execute_reply.started":"2021-12-27T03:48:41.850224Z","shell.execute_reply":"2021-12-27T03:48:41.873507Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"def unk_counter(sentence):\n    '''Counts the number of time UNK appears in a sentence.'''\n    unk_count = 0\n    for word in sentence:\n        if word == vocab_to_int[\"<UNK>\"]:\n            unk_count += 1\n    return unk_count","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:49:06.894510Z","iopub.execute_input":"2021-12-27T03:49:06.894800Z","iopub.status.idle":"2021-12-27T03:49:06.899760Z","shell.execute_reply.started":"2021-12-27T03:49:06.894766Z","shell.execute_reply":"2021-12-27T03:49:06.898817Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"max_text_length = 83 # This will cover up to 89.5% lengthes\nmax_summary_length = 13 # This will cover up to 99% lengthes\nmin_length = 2\nunk_text_limit = 1 # text can contain up to 1 UNK word\nunk_summary_limit = 0 # Summary should not contain any UNK word\n\ndef filter_condition(item):\n    int_summary = item[0]\n    int_text = item[1]\n    if(len(int_summary) >= min_length and \n       len(int_summary) <= max_summary_length and \n       len(int_text) >= min_length and \n       len(int_text) <= max_text_length and \n       unk_counter(int_summary) <= unk_summary_limit and \n       unk_counter(int_text) <= unk_text_limit):\n        return True\n    else:\n        return False\n\nint_text_summaries = list(zip(int_summaries , int_texts))\nint_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\nsorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\nsorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\nsorted_summaries = list(sorted_int_text_summaries[0])\nsorted_texts = list(sorted_int_text_summaries[1])\n# Delete those temporary varaibles\ndel int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n# Compare lengths to ensure they match\nprint(len(sorted_summaries))\nprint(len(sorted_texts))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:49:09.539537Z","iopub.execute_input":"2021-12-27T03:49:09.539816Z","iopub.status.idle":"2021-12-27T03:49:14.320051Z","shell.execute_reply.started":"2021-12-27T03:49:09.539786Z","shell.execute_reply":"2021-12-27T03:49:14.319144Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"lengths_texts = [len(text) for text in sorted_texts]\nlengths_texts[:20]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:49:18.549160Z","iopub.execute_input":"2021-12-27T03:49:18.549447Z","iopub.status.idle":"2021-12-27T03:49:18.641978Z","shell.execute_reply.started":"2021-12-27T03:49:18.549418Z","shell.execute_reply":"2021-12-27T03:49:18.641159Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"__pickleStuff(\"./data/clean_summaries.p\",clean_summaries)\n__pickleStuff(\"./data/clean_texts.p\",clean_texts)\n\n__pickleStuff(\"./data/sorted_summaries.p\",sorted_summaries)\n__pickleStuff(\"./data/sorted_texts.p\",sorted_texts)\n__pickleStuff(\"./data/word_embedding_matrix.p\",word_embedding_matrix)\n\n__pickleStuff(\"./data/vocab_to_int.p\",vocab_to_int)\n__pickleStuff(\"./data/int_to_vocab.p\",int_to_vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Building the Model","metadata":{}},{"cell_type":"code","source":"def model_inputs():\n    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n    lr = tf.placeholder(tf.float32, name='learning_rate')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n\n    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:50:47.499312Z","iopub.execute_input":"2021-12-27T03:50:47.500191Z","iopub.status.idle":"2021-12-27T03:50:47.506329Z","shell.execute_reply.started":"2021-12-27T03:50:47.500149Z","shell.execute_reply":"2021-12-27T03:50:47.505739Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"def process_encoding_input(target_data, vocab_to_int, batch_size):  \n    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n\n    return dec_input","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:51:03.840280Z","iopub.execute_input":"2021-12-27T03:51:03.840876Z","iopub.status.idle":"2021-12-27T03:51:03.845327Z","shell.execute_reply.started":"2021-12-27T03:51:03.840826Z","shell.execute_reply":"2021-12-27T03:51:03.844792Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n    for layer in range(num_layers):\n        with tf.variable_scope('encoder_{}'.format(layer)):\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n                                                    input_keep_prob = keep_prob)\n\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n                                                    input_keep_prob = keep_prob)\n\n            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n                                                                    cell_bw, \n                                                                    rnn_inputs,\n                                                                    sequence_length,\n                                                                    dtype=tf.float32)\n            enc_output = tf.concat(enc_output,2)\n            # original code is missing this line below, that is how we connect layers \n            # by feeding the current layer's output to next layer's input\n            rnn_inputs = enc_output\n    return enc_output, enc_state","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:51:14.786797Z","iopub.execute_input":"2021-12-27T03:51:14.787491Z","iopub.status.idle":"2021-12-27T03:51:14.795397Z","shell.execute_reply.started":"2021-12-27T03:51:14.787441Z","shell.execute_reply":"2021-12-27T03:51:14.794537Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n                            vocab_size, max_summary_length,batch_size):\n    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n                                                        sequence_length=summary_length,\n                                                        time_major=False)\n\n    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n                                                       helper=training_helper,\n                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n                                                       output_layer = output_layer)\n\n    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n                                                           output_time_major=False,\n                                                           impute_finished=True,\n                                                           maximum_iterations=max_summary_length)\n    return training_logits","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:51:25.016899Z","iopub.execute_input":"2021-12-27T03:51:25.017766Z","iopub.status.idle":"2021-12-27T03:51:25.024591Z","shell.execute_reply.started":"2021-12-27T03:51:25.017723Z","shell.execute_reply":"2021-12-27T03:51:25.023714Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n                             max_summary_length, batch_size):\n    '''Create the inference logits'''\n    \n    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n    \n    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n                                                                start_tokens,\n                                                                end_token)\n                \n    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n                                                        inference_helper,\n                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n                                                        output_layer)\n                \n    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n                                                            output_time_major=False,\n                                                            impute_finished=True,\n                                                            maximum_iterations=max_summary_length)\n    \n    return inference_logits","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:51:41.483824Z","iopub.execute_input":"2021-12-27T03:51:41.484134Z","iopub.status.idle":"2021-12-27T03:51:41.492130Z","shell.execute_reply.started":"2021-12-27T03:51:41.484100Z","shell.execute_reply":"2021-12-27T03:51:41.491500Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"def lstm_cell(lstm_size, keep_prob):\n    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n\ndef decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n    '''Create the decoding cell and attention for the training and inference decoding layers'''\n    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n                                                     enc_output,\n                                                     text_length,\n                                                     normalize=False,\n                                                     name='BahdanauAttention')\n    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n    with tf.variable_scope(\"decode\"):\n        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n                                                  output_layer,\n                                                  vocab_size,\n                                                  max_summary_length,\n                                                  batch_size)\n    with tf.variable_scope(\"decode\", reuse=True):\n        inference_logits = inference_decoding_layer(embeddings,\n                                                    vocab_to_int['<GO>'],\n                                                    vocab_to_int['<EOS>'],\n                                                    dec_cell,\n                                                    output_layer,\n                                                    max_summary_length,\n                                                    batch_size)\n    return training_logits, inference_logits","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:52:07.712849Z","iopub.execute_input":"2021-12-27T03:52:07.713422Z","iopub.status.idle":"2021-12-27T03:52:07.723359Z","shell.execute_reply.started":"2021-12-27T03:52:07.713371Z","shell.execute_reply":"2021-12-27T03:52:07.722729Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n    '''Use the previous functions to create the training and inference logits'''\n    \n    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n    embeddings = word_embedding_matrix\n    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n                                                        embeddings,\n                                                        enc_output,\n                                                        enc_state, \n                                                        vocab_size, \n                                                        text_length, \n                                                        summary_length, \n                                                        max_summary_length,\n                                                        rnn_size, \n                                                        vocab_to_int, \n                                                        keep_prob, \n                                                        batch_size,\n                                                        num_layers)\n    return training_logits, inference_logits","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:52:30.939772Z","iopub.execute_input":"2021-12-27T03:52:30.940467Z","iopub.status.idle":"2021-12-27T03:52:30.947373Z","shell.execute_reply.started":"2021-12-27T03:52:30.940422Z","shell.execute_reply":"2021-12-27T03:52:30.946800Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"def pad_sentence_batch(sentence_batch):\n    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n    max_sentence = max([len(sentence) for sentence in sentence_batch])\n    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:52:46.567313Z","iopub.execute_input":"2021-12-27T03:52:46.568365Z","iopub.status.idle":"2021-12-27T03:52:46.573035Z","shell.execute_reply.started":"2021-12-27T03:52:46.568323Z","shell.execute_reply":"2021-12-27T03:52:46.572430Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"def get_batches(summaries, texts, batch_size):\n    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n    for batch_i in range(0, len(texts)//batch_size):\n        start_i = batch_i * batch_size\n        summaries_batch = summaries[start_i:start_i + batch_size]\n        texts_batch = texts[start_i:start_i + batch_size]\n        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n        \n        # Need the lengths for the _lengths parameters\n        pad_summaries_lengths = []\n        for summary in pad_summaries_batch:\n            pad_summaries_lengths.append(len(summary))\n        \n        pad_texts_lengths = []\n        for text in pad_texts_batch:\n            pad_texts_lengths.append(len(text))\n        \n        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:52:48.552986Z","iopub.execute_input":"2021-12-27T03:52:48.553246Z","iopub.status.idle":"2021-12-27T03:52:48.560557Z","shell.execute_reply.started":"2021-12-27T03:52:48.553218Z","shell.execute_reply":"2021-12-27T03:52:48.559398Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\nsorted_summaries_samples = sorted_summaries[7:50]\nsorted_texts_samples = sorted_texts[7:50]\npad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n    sorted_summaries_samples, sorted_texts_samples, 5))\nprint(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:53:13.210518Z","iopub.execute_input":"2021-12-27T03:53:13.210976Z","iopub.status.idle":"2021-12-27T03:53:13.217133Z","shell.execute_reply.started":"2021-12-27T03:53:13.210940Z","shell.execute_reply":"2021-12-27T03:53:13.216526Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# Set the Hyperparameters\nepochs = 100\nbatch_size = 64\nrnn_size = 256\nnum_layers = 2\nlearning_rate = 0.005\nkeep_probability = 0.95","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:53:25.252274Z","iopub.execute_input":"2021-12-27T03:53:25.252831Z","iopub.status.idle":"2021-12-27T03:53:25.257303Z","shell.execute_reply.started":"2021-12-27T03:53:25.252792Z","shell.execute_reply":"2021-12-27T03:53:25.256280Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"# Build the graph\ntrain_graph = tf.Graph()\n# Set the graph to default to ensure that it is ready for training\nwith train_graph.as_default():\n    \n    # Load the model inputs    \n    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n\n    # Create the training and inference logits\n    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n                                                      targets, \n                                                      keep_prob,   \n                                                      text_length,\n                                                      summary_length,\n                                                      max_summary_length,\n                                                      len(vocab_to_int)+1,\n                                                      rnn_size, \n                                                      num_layers, \n                                                      vocab_to_int,\n                                                      batch_size)\n    \n    # Create tensors for the training logits and inference logits\n    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n    \n    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n\n    with tf.name_scope(\"optimization\"):\n        # Loss function\n        cost = tf.contrib.seq2seq.sequence_loss(\n            training_logits,\n            targets,\n            masks)\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)\nprint(\"Graph is built.\")\ngraph_location = \"./graph\"\nprint(graph_location)\ntrain_writer = tf.summary.FileWriter(graph_location)\ntrain_writer.add_graph(train_graph)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training the Model","metadata":{}},{"cell_type":"code","source":"# Subset the data for training\nstart = 200000\nend = start + 50000\nsorted_summaries_short = sorted_summaries[start:end]\nsorted_texts_short = sorted_texts[start:end]\nprint(\"The shortest text length:\", len(sorted_texts_short[0]))\nprint(\"The longest text length:\",len(sorted_texts_short[-1]))","metadata":{"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"learning_rate_decay = 0.95\nmin_learning_rate = 0.0005\ndisplay_step = 20 # Check training loss after every 20 batches\nstop_early = 0 \nstop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\nper_epoch = 3 # Make 3 update checks per epoch\nupdate_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n\nupdate_loss = 0 \nbatch_loss = 0\nsummary_update_loss = [] # Record the update losses for saving improvements in the model\n\ncheckpoint = \"best_model.ckpt\" \nwith tf.Session(graph=train_graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    # If we want to continue training a previous session\n    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n    #loader.restore(sess, checkpoint)\n    \n    for epoch_i in range(1, epochs+1):\n        update_loss = 0\n        batch_loss = 0\n        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n            start_time = time.time()\n            _, loss = sess.run(\n                [train_op, cost],\n                {input_data: texts_batch,\n                 targets: summaries_batch,\n                 lr: learning_rate,\n                 summary_length: summaries_lengths,\n                 text_length: texts_lengths,\n                 keep_prob: keep_probability})\n\n            batch_loss += loss\n            update_loss += loss\n            end_time = time.time()\n            batch_time = end_time - start_time\n\n            if batch_i % display_step == 0 and batch_i > 0:\n                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n                      .format(epoch_i,\n                              epochs, \n                              batch_i, \n                              len(sorted_texts_short) // batch_size, \n                              batch_loss / display_step, \n                              batch_time*display_step))\n                batch_loss = 0\n\n            if batch_i % update_check == 0 and batch_i > 0:\n                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n                summary_update_loss.append(update_loss)\n                \n                # If the update loss is at a new minimum, save the model\n                if update_loss <= min(summary_update_loss):\n                    print('New Record!') \n                    stop_early = 0\n                    saver = tf.train.Saver() \n                    saver.save(sess, checkpoint)\n\n                else:\n                    print(\"No Improvement.\")\n                    stop_early += 1\n                    if stop_early == stop:\n                        break\n                update_loss = 0\n            \n                    \n        # Reduce learning rate, but not below its minimum value\n        learning_rate *= learning_rate_decay\n        if learning_rate < min_learning_rate:\n            learning_rate = min_learning_rate\n        \n        if stop_early == stop:\n            print(\"Stopping Training.\")\n            break\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Making Our Own Summaries","metadata":{}},{"cell_type":"code","source":"def text_to_seq(text):\n    '''Prepare the text for the model'''\n    \n    text = clean_text(text)\n    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T04:05:16.061724Z","iopub.execute_input":"2021-12-27T04:05:16.062034Z","iopub.status.idle":"2021-12-27T04:05:16.067163Z","shell.execute_reply.started":"2021-12-27T04:05:16.062002Z","shell.execute_reply":"2021-12-27T04:05:16.066274Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n               \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\ngeneragte_summary_length =  [3,2]\n\ntexts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\ncheckpoint = \"./best_model.ckpt\"\nif type(generagte_summary_length) is list:\n    if len(input_sentences)!=len(generagte_summary_length):\n        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n    generagte_summary_length_list = generagte_summary_length\nelse:\n    generagte_summary_length_list = [generagte_summary_length] * len(texts)\nloaded_graph = tf.Graph()\nwith tf.Session(graph=loaded_graph) as sess:\n    # Load saved model\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n    loader.restore(sess, checkpoint)\n    input_data = loaded_graph.get_tensor_by_name('input:0')\n    logits = loaded_graph.get_tensor_by_name('predictions:0')\n    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n    #Multiply by batch_size to match the model's input parameters\n    for i, text in enumerate(texts):\n        generagte_summary_length = generagte_summary_length_list[i]\n        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n                                          text_length: [len(text)]*batch_size,\n                                          keep_prob: 1.0})[0] \n        # Remove the padding from the summaries\n        pad = vocab_to_int[\"<PAD>\"] \n        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thank You**","metadata":{}}]}